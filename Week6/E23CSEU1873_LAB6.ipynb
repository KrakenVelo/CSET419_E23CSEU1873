{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igaQddvnd8mc",
        "outputId": "2250da08-faa4-41cf-da38-b34da80a14bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n",
            "Downloading dataset...\n",
            "Extracting dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1131/552673850.py:37: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Ready\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1131/552673850.py:187: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_G=torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-1131/552673850.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_D=torch.cuda.amp.GradScaler()\n",
            "  0%|          | 0/750 [00:00<?, ?it/s]/tmp/ipython-input-1131/552673850.py:221: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-1131/552673850.py:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch [1/10]: 100%|██████████| 750/750 [00:29<00:00, 25.50it/s, D_loss=0.617, G_loss=19.4]\n",
            "Epoch [2/10]: 100%|██████████| 750/750 [00:27<00:00, 27.56it/s, D_loss=0.318, G_loss=13.1]\n",
            "Epoch [3/10]: 100%|██████████| 750/750 [00:27<00:00, 27.73it/s, D_loss=0.519, G_loss=16.8]\n",
            "Epoch [4/10]: 100%|██████████| 750/750 [00:27<00:00, 27.56it/s, D_loss=0.277, G_loss=16.9]\n",
            "Epoch [5/10]: 100%|██████████| 750/750 [00:27<00:00, 27.30it/s, D_loss=1.04, G_loss=23.8]\n",
            "Epoch [6/10]: 100%|██████████| 750/750 [00:27<00:00, 27.02it/s, D_loss=0.556, G_loss=11.7]\n",
            "Epoch [7/10]: 100%|██████████| 750/750 [00:27<00:00, 27.34it/s, D_loss=0.602, G_loss=17.8]\n",
            "Epoch [8/10]: 100%|██████████| 750/750 [00:27<00:00, 27.45it/s, D_loss=0.467, G_loss=14]\n",
            "Epoch [9/10]: 100%|██████████| 750/750 [00:27<00:00, 27.63it/s, D_loss=0.505, G_loss=17.1]\n",
            "Epoch [10/10]: 100%|██████████| 750/750 [00:27<00:00, 27.60it/s, D_loss=0.626, G_loss=8.13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Complete. Check /results folder for Pix2Pix outputs.\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# PIX2PIX FULL IMPLEMENTATION\n",
        "# ==============================\n",
        "\n",
        "!pip -q install torchvision pillow tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)\n",
        "\n",
        "# ======================================================\n",
        "# 1. DOWNLOAD DATASET (Edges2Shoes)\n",
        "# ======================================================\n",
        "\n",
        "dataset_url = \"https://efrosgans.eecs.berkeley.edu/pix2pix/datasets/edges2shoes.tar.gz\"\n",
        "dataset_file = \"edges2shoes.tar.gz\"\n",
        "\n",
        "if not os.path.exists(dataset_file):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(dataset_url, dataset_file)\n",
        "\n",
        "if not os.path.exists(\"edges2shoes\"):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with tarfile.open(dataset_file, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "\n",
        "print(\"Dataset Ready\")\n",
        "\n",
        "# ======================================================\n",
        "# 2. DATASET CLASS\n",
        "# ======================================================\n",
        "\n",
        "class Pix2PixDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.files = os.listdir(root_dir)\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((128,128)),   # faster training\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.files[index])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        w, h = img.size\n",
        "        w2 = w // 2\n",
        "\n",
        "        edge = img.crop((0,0,w2,h))\n",
        "        real = img.crop((w2,0,w,h))\n",
        "\n",
        "        edge = self.transform(edge)\n",
        "        real = self.transform(real)\n",
        "\n",
        "        return edge, real\n",
        "\n",
        "# ======================================================\n",
        "# 3. DATALOADERS\n",
        "# ======================================================\n",
        "\n",
        "train_dataset = Pix2PixDataset(\"edges2shoes/train\")\n",
        "train_dataset = Subset(train_dataset, range(3000))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "test_dataset = Pix2PixDataset(\"edges2shoes/val\")\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ======================================================\n",
        "# 4. U-NET GENERATOR\n",
        "# ======================================================\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, normalize=True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm2d(out_c))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "    def forward(self,x): return self.block(x)\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_c,out_c,4,2,1,bias=False),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "    def forward(self,x,skip):\n",
        "        x=self.block(x)\n",
        "        return torch.cat((x,skip),1)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d1=DownBlock(3,64,False)\n",
        "        self.d2=DownBlock(64,128)\n",
        "        self.d3=DownBlock(128,256)\n",
        "        self.d4=DownBlock(256,512)\n",
        "        self.d5=DownBlock(512,512)\n",
        "        self.d6=DownBlock(512,512)\n",
        "\n",
        "        self.u1=UpBlock(512,512)\n",
        "        self.u2=UpBlock(1024,512)\n",
        "        self.u3=UpBlock(1024,256)\n",
        "        self.u4=UpBlock(512,128)\n",
        "        self.u5=UpBlock(256,64)\n",
        "\n",
        "        self.final=nn.Sequential(\n",
        "            nn.ConvTranspose2d(128,3,4,2,1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        d1=self.d1(x)\n",
        "        d2=self.d2(d1)\n",
        "        d3=self.d3(d2)\n",
        "        d4=self.d4(d3)\n",
        "        d5=self.d5(d4)\n",
        "        d6=self.d6(d5)\n",
        "\n",
        "        u1=self.u1(d6,d5)\n",
        "        u2=self.u2(u1,d4)\n",
        "        u3=self.u3(u2,d3)\n",
        "        u4=self.u4(u3,d2)\n",
        "        u5=self.u5(u4,d1)\n",
        "\n",
        "        return self.final(u5)\n",
        "\n",
        "# ======================================================\n",
        "# 5. PATCHGAN DISCRIMINATOR\n",
        "# ======================================================\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        def block(in_c,out_c,normalize=True):\n",
        "            layers=[nn.Conv2d(in_c,out_c,4,2,1)]\n",
        "            if normalize: layers.append(nn.BatchNorm2d(out_c))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "            return layers\n",
        "\n",
        "        self.model=nn.Sequential(\n",
        "            *block(6,64,False),\n",
        "            *block(64,128),\n",
        "            *block(128,256),\n",
        "            *block(256,512),\n",
        "            nn.Conv2d(512,1,4,1,1)\n",
        "        )\n",
        "    def forward(self,edge,real):\n",
        "        x=torch.cat([edge,real],1)\n",
        "        return self.model(x)\n",
        "\n",
        "G=Generator().to(device)\n",
        "D=Discriminator().to(device)\n",
        "\n",
        "# ======================================================\n",
        "# 6. LOSSES & OPTIMIZERS\n",
        "# ======================================================\n",
        "\n",
        "criterion_GAN=nn.BCEWithLogitsLoss()\n",
        "criterion_L1=nn.L1Loss()\n",
        "lambda_L1=100\n",
        "\n",
        "opt_G=optim.Adam(G.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
        "opt_D=optim.Adam(D.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
        "\n",
        "scaler_G=torch.cuda.amp.GradScaler()\n",
        "scaler_D=torch.cuda.amp.GradScaler()\n",
        "\n",
        "# ======================================================\n",
        "# 7. SAVE OUTPUT IMAGES\n",
        "# ======================================================\n",
        "\n",
        "os.makedirs(\"results\",exist_ok=True)\n",
        "\n",
        "def save_examples(epoch):\n",
        "    G.eval()\n",
        "    edge,real=next(iter(test_loader))\n",
        "    edge=edge.to(device)\n",
        "    real=real.to(device)\n",
        "    with torch.no_grad():\n",
        "        fake=G(edge)\n",
        "    grid=torch.cat([edge,fake,real],0)\n",
        "    grid=(grid+1)/2\n",
        "    vutils.save_image(grid,f\"results/epoch_{epoch}.png\",nrow=edge.size(0))\n",
        "    G.train()\n",
        "\n",
        "# ======================================================\n",
        "# 8. TRAIN PIX2PIX\n",
        "# ======================================================\n",
        "\n",
        "epochs=10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loop=tqdm(train_loader)\n",
        "    for i,(edge,real) in enumerate(loop):\n",
        "        edge=edge.to(device)\n",
        "        real=real.to(device)\n",
        "\n",
        "        if i%2==0:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                fake=G(edge)\n",
        "                D_real=D(edge,real)\n",
        "                D_fake=D(edge,fake.detach())\n",
        "                loss_real=criterion_GAN(D_real,torch.ones_like(D_real))\n",
        "                loss_fake=criterion_GAN(D_fake,torch.zeros_like(D_fake))\n",
        "                loss_D=(loss_real+loss_fake)/2\n",
        "\n",
        "            opt_D.zero_grad()\n",
        "            scaler_D.scale(loss_D).backward()\n",
        "            scaler_D.step(opt_D)\n",
        "            scaler_D.update()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake=G(edge)\n",
        "            D_fake=D(edge,fake)\n",
        "            loss_GAN=criterion_GAN(D_fake,torch.ones_like(D_fake))\n",
        "            loss_L1=criterion_L1(fake,real)\n",
        "            loss_G=loss_GAN+lambda_L1*loss_L1\n",
        "\n",
        "        opt_G.zero_grad()\n",
        "        scaler_G.scale(loss_G).backward()\n",
        "        scaler_G.step(opt_G)\n",
        "        scaler_G.update()\n",
        "\n",
        "        loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
        "        loop.set_postfix(D_loss=loss_D.item(),G_loss=loss_G.item())\n",
        "\n",
        "    save_examples(epoch+1)\n",
        "\n",
        "# ======================================================\n",
        "# 9. BASELINE CNN (FOR COMPARISON)\n",
        "# ======================================================\n",
        "\n",
        "class CNNBaseline(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder=nn.Sequential(\n",
        "            nn.Conv2d(3,64,4,2,1),nn.ReLU(),\n",
        "            nn.Conv2d(64,128,4,2,1),nn.ReLU(),\n",
        "            nn.Conv2d(128,256,4,2,1),nn.ReLU()\n",
        "        )\n",
        "        self.decoder=nn.Sequential(\n",
        "            nn.ConvTranspose2d(256,128,4,2,1),nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128,64,4,2,1),nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64,3,4,2,1),nn.Tanh()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "baseline=CNNBaseline().to(device)\n",
        "opt=optim.Adam(baseline.parameters(),lr=0.0002)\n",
        "criterion=nn.L1Loss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    for edge,real in train_loader:\n",
        "        edge=edge.to(device)\n",
        "        real=real.to(device)\n",
        "        output=baseline(edge)\n",
        "        loss=criterion(output,real)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "print(\"Training Complete. Check /results folder for Pix2Pix outputs.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test images:\", len(test_dataset))\n",
        "e, r = next(iter(test_loader))\n",
        "print(\"Edge shape:\", e.shape)\n",
        "print(\"Real shape:\", r.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_EI6wvV1_iD",
        "outputId": "dae12fd2-a31f-4cbb-b60a-faf1048f3857"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test images: 200\n",
            "Edge shape: torch.Size([4, 3, 128, 128])\n",
            "Real shape: torch.Size([4, 3, 128, 128])\n"
          ]
        }
      ]
    }
  ]
}